{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc59a96-9705-492f-a43a-86d42c84bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading models...\n",
      "🔄 Loading known faces...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'https://res.cloudinary.com/ddoeialig/image/upload/v1739511233/uploads/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Loading known faces...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m known_face_encodings, known_face_names \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(known_faces_dir):\n\u001b[0;32m     39\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(known_faces_dir, filename)\n\u001b[0;32m     40\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filepath)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'https://res.cloudinary.com/ddoeialig/image/upload/v1739511233/uploads/'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import threading\n",
    "from queue import Queue\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "from fer import FER\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "# MongoDB setup\n",
    "client = MongoClient(\"mongodb+srv://admin-aesha:zaP6CjyjK8WpnG7@cluster0.ka7ha.mongodb.net/?retryWrites=true&w=majority\")\n",
    "\n",
    "\n",
    "db = client[\"CCTV\"]\n",
    "collection = db[\"activity_log\"]\n",
    "\n",
    "# Paths to models\n",
    "known_faces_dir = \"https://res.cloudinary.com/ddoeialig/image/upload/v1739511233/uploads/\"\n",
    "shape_predictor_path = \"C:/Users/aesha/Downloads/CCTV VIDEO DETECTION/shape_predictor_68_face_landmarks.dat\"\n",
    "face_rec_model_path = \"C:/Users/aesha/Downloads/CCTV VIDEO DETECTION/dlib_face_recognition_resnet_model_v1.dat\"\n",
    "\n",
    "# Load models\n",
    "print(\"🔄 Loading models...\")\n",
    "detector = dlib.get_frontal_face_detector()  \n",
    "shape_predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "face_rec_model = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "\n",
    "# Load emotion detector\n",
    "emotion_detector = FER()\n",
    "\n",
    "# Load known faces\n",
    "print(\"🔄 Loading known faces...\")\n",
    "known_face_encodings, known_face_names = [], []\n",
    "for filename in os.listdir(known_faces_dir):\n",
    "    filepath = os.path.join(known_faces_dir, filename)\n",
    "    image = cv2.imread(filepath)\n",
    "    if image is None:\n",
    "        continue\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = detector(rgb_image, 1)  \n",
    "    if len(faces) == 0:\n",
    "        continue\n",
    "    shape = shape_predictor(rgb_image, faces[0])\n",
    "    face_encoding = np.array(face_rec_model.compute_face_descriptor(rgb_image, shape))\n",
    "    known_face_encodings.append(face_encoding)\n",
    "    known_face_names.append(os.path.splitext(filename)[0])\n",
    "\n",
    "known_face_encodings = np.array(known_face_encodings)\n",
    "\n",
    "# ** RTSP Camera Setup **\n",
    "rtsp_url = \"rtsp://admin:RoadRacer%401.@117.99.109.118:554/cam/realmonitor?channel=1&subtype=0\"\n",
    "cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  \n",
    "cap.set(cv2.CAP_PROP_FPS, 50)  \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  \n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  \n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Error: Could not connect to RTSP stream.\")\n",
    "    exit()\n",
    "\n",
    "print(\"✅ RTSP stream connected successfully.\")\n",
    "\n",
    "# ** Multi-threading Setup **\n",
    "frame_queue = Queue(maxsize=5)  \n",
    "detection_count = {}  # Tracks number of times each person is detected\n",
    "frame_skip = 2  \n",
    "frame_count = 0\n",
    "\n",
    "# ** Function to Play Welcome Message **\n",
    "def play_welcome_message(name, activity):\n",
    "    message = f\"{name} is {activity}\"\n",
    "    file_path = \"C:/Users/Aesha/Downloads/welcome.mp3\"\n",
    "    \n",
    "    try:\n",
    "        tts = gTTS(text=message, lang='en')\n",
    "        tts.save(file_path)\n",
    "        playsound.playsound(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during speech generation or playback: {e}\")\n",
    "    finally:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)  \n",
    "\n",
    "# ** Function to Save to Database **\n",
    "def save_to_database(name, emotion):\n",
    "    global detection_count\n",
    "    \n",
    "    if name not in detection_count:\n",
    "        detection_count[name] = 1\n",
    "    else:\n",
    "        detection_count[name] += 1\n",
    "\n",
    "    # Alternate activity based on detection count\n",
    "    activity = \"entering\" if detection_count[name] % 2 == 1 else \"exiting\"\n",
    "\n",
    "    data = {\"name\": name, \"activity\": activity, \"emotion\": emotion, \"timestamp\": datetime.now()}\n",
    "    collection.insert_one(data)\n",
    "    print(f\"✅ Stored: {name} | Activity: '{activity}' | Emotion: '{emotion}' | Time: {data['timestamp']}\")\n",
    "\n",
    "    play_welcome_message(name, activity)\n",
    "\n",
    "# ** Function to Process Frames (Separate Thread) **\n",
    "def process_frame():\n",
    "    while True:\n",
    "        if not frame_queue.empty():\n",
    "            frame = frame_queue.get()\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            faces = detector(rgb_frame, 0)  \n",
    "\n",
    "            for face in faces:\n",
    "                shape = shape_predictor(rgb_frame, face)\n",
    "                face_encoding = np.array(face_rec_model.compute_face_descriptor(rgb_frame, shape))\n",
    "\n",
    "                if known_face_encodings.size > 0:\n",
    "                    distances = np.linalg.norm(known_face_encodings - face_encoding, axis=1)\n",
    "                    min_distance_index = np.argmin(distances)\n",
    "                    if distances[min_distance_index] < 0.45:\n",
    "                        name = known_face_names[min_distance_index]\n",
    "                    else:\n",
    "                        name = \"Unknown\"\n",
    "                else:\n",
    "                    name = \"Unknown\"\n",
    "\n",
    "                x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "                # ** Emotion Detection **\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "                if face_roi.size > 0:\n",
    "                    emotion, score = emotion_detector.top_emotion(face_roi)\n",
    "                    if emotion is None:\n",
    "                        emotion = \"Neutral\"\n",
    "                    print(f\"🧠 Emotion detected: {emotion} (Score: {score})\")\n",
    "                else:\n",
    "                    emotion = \"Neutral\"\n",
    "\n",
    "                # ** Store Data & Play Welcome Message **\n",
    "                if name != \"Unknown\":\n",
    "                    save_to_database(name, emotion)\n",
    "\n",
    "            cv2.imshow(\"📷 Real-Time Face Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# ** Start Processing Thread **\n",
    "threading.Thread(target=process_frame, daemon=True).start()\n",
    "\n",
    "# ** Video Streaming Loop (Optimized) **\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠ Warning: Failed to read frame from RTSP. Retrying...\")\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue  \n",
    "\n",
    "    if not frame_queue.full():\n",
    "        frame_queue.put(frame)  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930523c-6665-49c4-b4c8-a057d5899885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
